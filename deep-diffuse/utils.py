import pickle
import numpy as np
import configparser
import pandas as pd
import networkx as nx
from os.path import join, isfile


# æ„å»ºè®ºæ–‡ä¸­ä½¿ç”¨çš„æ—¶é—´é—´éš”
def process_timestamps(timestamps):
    arr = np.asarray(timestamps)
    # åšå·® nä¸ªå…ƒç´ å˜æˆn - 1ä¸ªå…ƒç´ 
    diff = list(np.diff(arr))
    # å°†æ—¶é—´é—´éš”è½¬æ¢ä¸ºåˆ†é’Ÿ
    diff = [d / 60.0 for d in diff]

    # è¿™é‡Œå¤„ç†é‡å¤çš„å·®å€¼ä¹Ÿä¸æ˜¯å•Šï¼Ÿå†…å±‚å¾ªç¯æœ‰å•¥ä½œç”¨ğŸ˜€
    for i in range(0, len(diff)-1):
        if diff[i] == diff[i+1]:
            for j in range(0, len(diff)):
                diff[j] += 0.25
    return diff

'''def write_seen_nodes(data_path, seq_len):
    seen_nodes = []
    with open(join(data_path, 'train.txt'), 'r') as read_file:
        for i, line in enumerate(read_file):
            query, cascade = line.strip().split(' ', 1)
            sequence = cascade.split(' ')[::2]
            seen_nodes.extend(sequence)
    with open(join(data_path, 'test.txt'), 'r') as read_file:
        for i, line in enumerate(read_file):
            query, cascade = line.strip().split(' ', 1)
            sequence = cascade.split(' ')[::2]
            seen_nodes.extend(sequence)
    seen_nodes = set(seen_nodes)
    with open(join(data_path, 'seen_nodes.txt'), 'w+') as write_file:
        for node in seen_nodes:
            write_file.write(node + '\n')
    print(len(seen_nodes))'''


def load_graph(data_path):
    # åŠ è½½èŠ‚ç‚¹ï¼ˆè·¯å¾„æ‹¼æ¥ï¼‰data/seen_nodes.txt
    node_file = join(data_path, 'seen_nodes.txt')
    with open(node_file, 'r') as f:
        seen_nodes = [int(x.strip()) for x in f]

    print(f"æœ€å¤§æ•°", max(seen_nodes));
    print(f"æœ€å°æ•°", min(seen_nodes));
    print(f"æ•°é‡", len(seen_nodes));


    print(type(seen_nodes))
    #seen_nodesæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé‡Œé¢æ˜¯æ‰€æœ‰çš„nodeèŠ‚ç‚¹
    # builds node index
    node_index = {v: i for i, v in enumerate(seen_nodes)}
    print(node_index)
    print(type(node_index))

    # æ„å»ºä¸€ä¸ªå­—å…¸ï¼š{èŠ‚ç‚¹ç¼–å·ï¼šèŠ‚ç‚¹ç¼–å·åœ¨seen_nodesä¸­çš„ç´¢å¼•}

    # loads graph
    '''graph_file = join(data_path, 'graph.txt')
    pkl_file = join(data_path, 'graph.pkl')

    if isfile(pkl_file):
        G = pickle.load(open(pkl_file, 'rb'))
    else:
        G = nx.Graph()
        G.name = data_path
        n_nodes = len(node_index)
        G.add_nodes_from(range(n_nodes))
        with open(graph_file, 'r') as f:
            next(f)
            for line in f:
                u, v = map(int, line.strip().split())
                if (u in node_index) and (v in node_index):
                    u = node_index[u]
                    v = node_index[v]
                    G.add_edge(u, v)
        pickle.dump(G, open(pkl_file, 'wb+'))'''

    return node_index


def load_instances(data_path, file_type, node_index, seq_len, limit, ratio=1.0, testing=False):
    '''
        ç”Ÿæˆä¸€ä¸ªè®­ç»ƒé›†ï¼ˆæµ‹è¯•é›†ï¼‰listï¼Œå…¶ä¸­åŒ…æ‹¬å¤šæ¡çº§è”ç»“æ„ä½“
        [
            { 
                'sequence': prefix_c, æ³¨æ„è¿™é‡Œå­˜å‚¨çš„æ˜¯èŠ‚ç‚¹ç´¢å¼•list
                'time': prefix_t, æ—¶é—´é—´éš”list
                'label_n': label_n,
                'label_t': label_t
            },
            { 
                'sequence': prefix_c, æ³¨æ„è¿™é‡Œå­˜å‚¨çš„æ˜¯èŠ‚ç‚¹ç´¢å¼•list
                'time': prefix_t, æ—¶é—´é—´éš”list
                'label_n': label_n,
                'label_t': label_t
            },
            ...
        ]
    '''
    max_diff = 0
    pkl_path = join(data_path, file_type + '.pkl')
    if isfile(pkl_path):
        instances = pickle.load(open(pkl_path, 'rb'))
    else: 
        file_name = join(data_path, file_type + '.txt')
        instances = []
        with open(file_name, 'r') as read_file:
            for i, line in enumerate(read_file):
                query, cascade = line.strip().split(' ', 1)
                # ä»ä¸‹æ ‡ä¸º0å¼€å§‹æ¯æ¬¡éš”2ä¸ªå…ƒç´ å–å‡ºä¸€ä¸ª
                cascade_nodes = list(map(int, cascade.split(' ')[::2]))
                # ä»ä¸‹æ ‡ä¸º1å¼€å§‹æ¯æ¬¡éš”2ä¸ªå…ƒç´ å–å‡ºä¸€ä¸ª
                cascade_times = list(map(float, cascade.split(' ')[1::2]))
                if seq_len is not None:

                    '''
                        ä¸ºä»€ä¹ˆè¿™é‡Œæ—¶é—´æˆ³çš„ä¸ªæ•°è¦æ¯”èŠ‚ç‚¹ä¸ªæ•°å¤šä¸€ä¸ªï¼Ÿï¼ˆä¸æ˜¯ä¸€æ ·çš„å—ï¼‰

                        å› ä¸ºæœ€åä½¿ç”¨çš„æ˜¯æ—¶é—´æˆ³çš„å·®å€¼ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤å·®å€¼ä¸ªæ•° == æ—¶é—´æˆ³ä¸ªæ•° - 1
                        ä¸ºäº†èŠ‚ç‚¹ä¸ªæ•°å’Œå·®å€¼ä¸ªæ•°ç›¸ç­‰ï¼Œæ‰€ä»¥æ—¶é—´æˆ³ä¸ªæ•°è¦æ¯”èŠ‚ç‚¹ä¸ªæ•°å¤šä¸€ä¸ª(?)

                    '''
                    cascade_nodes = cascade_nodes[:seq_len+1]
                    cascade_times = cascade_times[:seq_len+2]
                    if len(cascade_times) == len(cascade_nodes):
                        cascade_nodes.pop()
                    cascade_times = process_timestamps(cascade_times) #å¤„ç†æ—¶é—´æˆ³

                    # å¦‚æœç¨‹åºä¸æ»¡è¶³è¿™ä¸ªæ¡ä»¶len(cascade_nodes) == len(cascade_timesï¼Œé‚£ä¹ˆå°±ä¼šæŠ¥é”™
                    assert len(cascade_nodes) == len(cascade_times), "Error: {} != {}".format(len(cascade_nodes), len(cascade_times))

                # å°†èŠ‚ç‚¹ç¼–å·è½¬æ¢ä¸ºèŠ‚ç‚¹ç´¢å¼•ï¼ˆèŠ‚ç‚¹ç´¢å¼•åº”è¯¥æ˜¯å¯¹åº”ä¸€ä¸ªèŠ‚ç‚¹çš„åµŒå…¥ï¼‰
                cascade_nodes = [node_index[x] for x in cascade_nodes]
                if not cascade_nodes or not cascade_times:
                    continue
                
                # max_diffåº”è¯¥æ˜¯æœ€å¤§çº§è”é•¿åº¦
                max_diff = max(max_diff, max(cascade_times))
                '''
                     ç”Ÿæˆä¸€ä¸ªlistï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€æ¡çº§è”ç»“æ„ä½“

                     [{ 'sequence': prefix_c, æ³¨æ„è¿™é‡Œå­˜å‚¨çš„æ˜¯èŠ‚ç‚¹ç´¢å¼•list
                       'time': prefix_t, æ—¶é—´é—´éš”list
                       'label_n': label_n,
                       'label_t': label_t
                       }
                    ]
                '''

                ins = process_cascade(cascade_nodes, cascade_times, testing)
                #æ³¨æ„è¿™é‡Œæ˜¯extend, è¡¨ç¤ºæ˜¯æ‰©å±•ï¼Œä¸æ˜¯append
                instances.extend(ins)

                if limit is not None and i == limit:
                    break
        # pickle.dump(instances, open(pkl_path, 'wb+'))
    total_samples = len(instances)
    #éšæœºé€‰æ‹©æ ·æœ¬ç´¢å¼•
    indices = np.random.choice(total_samples, int(total_samples * ratio), replace=False)
    sampled_instances = [instances[i] for i in indices]
    return sampled_instances, max_diff



def process_cascade(cascade, timestamps, testing=False):
    size = len(cascade)
    examples = []
    for i, node in enumerate(cascade):
        if i == size - 1 and not testing:
            return examples
        if i < size - 1 and testing:
            continue
        prefix_c = cascade[: i + 1]
        prefix_t = timestamps[: i + 1]
        # predecessors = set(network[node]) & set(prefix_c)
        # others = set(prefix_c).difference(set(predecessors))

        '''if i == 0:
            times.extend([0.0])
        else:
            # print(i)
            times.extend([(timestamps[i-1] - timestamps[i])])'''

        if not testing:
            label_n = cascade[i + 1] # æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ç´¢å¼• ï¼ˆä¸æ˜¯å›åˆ°è‡ªå·±äº†å—ï¼Ÿï¼‰
            label_t = timestamps[i+1] #æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´æˆ³
        else:
            label_n = None
            label_t = None

        example = {'sequence': prefix_c, 'time': prefix_t,
                   'label_n': label_n, 'label_t': label_t}

        if not testing:
            examples.append(example)
        else:
            return example


#åŠ è½½å‚æ•°å‡½æ•°
def load_params(param_file='params.ini'):
    options = {}
    config = configparser.ConfigParser()
    config.read(param_file)
    #æ•°æ®æ–‡ä»¶å¤¹
    options['data_dir'] = config['general']['data_dir']
    #æ•°æ®é›†çš„åå­—
    options['dataset_name'] = config['general']['dataset_name']
    #æ¯æ¬¡è®­ç»ƒçš„æ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
    options['batch_size'] = int(config['general']['batch_size'])
    #åºåˆ—é•¿åº¦ï¼Œæ¯æ¬¡è®­ç»ƒæ—¶å¤„ç†çš„æ—¶é—´æ­¥æ•°
    options['seq_len'] = int(config['general']['seq_len'])
    #çª—å£é•¿åº¦ - åŠ å¼ºç‰ˆï¼Œè£å‰ªåºåˆ—é•¿åº¦ç”¨çš„
    options['win_len'] = int(config['general']['win_len'])
    #å•å…ƒç±»å‹ï¼ˆä¸çŸ¥é“å¹²å•¥çš„ï¼Ÿï¼‰
    options['cell_type'] = str(config['general']['cell_type'])
    #è®­ç»ƒçš„è½®æ•°
    options['epochs'] = int(config['general']['epochs'])
    #å­¦ä¹ ç‡
    options['learning_rate'] = float(config['general']['learning_rate'])
    #åº”è¯¥æ˜¯éšè—å±‚çš„å¤§å°ï¼ˆç»´åº¦è¿˜æ˜¯æ•°é‡ï¼Ÿï¼‰
    options['state_size'] = int(config['general']['state_size'])
    #æ˜¯å¦è¿›è¡ŒèŠ‚ç‚¹é¢„æµ‹
    options['node_pred'] = config.getboolean('general','node_pred')
    #æ˜¯å¦åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±æ•°æ®ï¼Ÿ
    options['shuffle'] = config.getboolean('general', 'shuffle')
    #åµŒå…¥å±‚çš„ç»´åº¦å¤§å° - 256
    options['embedding_size'] = int(config['general']['embedding_size'])
    #é‡‡æ ·æ•°é‡ï¼ˆé€‰æ‰¹çš„æ—¶å€™ï¼‰
    options['n_samples'] = int(config['general']['n_samples'])
    #æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
    options['use_attention'] = config.getboolean('general', 'use_attention')
    #æ—¶é—´æŸå¤±å‡½æ•°ï¼ˆå‡æ–¹è¯¯å·®mseï¼‰
    options['time_loss'] = str(config['general']['time_loss'])
    #?
    options['num_glimpse'] = int(config['general']['num_glimpse'])
    #ï¼Ÿéšè—å±‚çš„å¤§å°
    options['hl_size'] = int(config['general']['h_size'])
    #ï¼Ÿéšè—å±‚çš„å¤§å°
    options['hg_size'] = int(config['general']['h_size'])
    #ï¼Ÿæ‰€æœ‰hlåˆæˆçš„hgçš„å¤§å°
    options['g_size'] = int(config['general']['g_size'])
    #ä½ç½®åµŒå…¥çš„ç»´åº¦
    options['loc_dim'] = int(config['general']['loc_dim'])
    #æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    options['clipping_val'] = float(config['general']['clipping_val'])
    #?: æœ€å°å­¦ä¹ ç‡ï¼Œç”¨äºå­¦ä¹ ç‡è°ƒåº¦
    options['min_lr'] = float(config['general']['min_lr'])
    #æµ‹è¯•é¢‘ç‡ï¼Œè¡¨ç¤ºæ¯å¤šå°‘ä¸ªepochè¿›è¡Œä¸€æ¬¡æµ‹è¯•ï¼ˆä¸è¿›è¡Œè®­ç»ƒï¼Œåªæ˜¯æµ‹è¯•ï¼‰
    options['test_freq'] = int(config['general']['test_freq'])
    #æ˜¾ç¤ºé¢‘ç‡ï¼Œè¡¨ç¤ºæ¯å¤šå°‘ä¸ªepochæ˜¾ç¤ºä¸€æ¬¡è®­ç»ƒè¿›åº¦ï¼ˆè®­ç»ƒçš„æŸå¤±ï¼‰
    options['disp_freq'] = int(config['general']['disp_freq'])

    return options


def prepare_minibatch(tuples, inference=False, options=None):
    '''
        å‡†å¤‡ä¸€ä¸ªbatchçš„æ•°æ®
        è¾“å…¥ï¼š
        tuples: å·²ç»ç»è¿‡é‡‡æ ·çš„samplesçš„çº§è”list
        [
                    1ä¸ªçº§è”
                    {  'sequence': prefix_c, æ³¨æ„è¿™é‡Œå­˜å‚¨çš„æ˜¯èŠ‚ç‚¹ç´¢å¼•
                       'time': prefix_t,
                       'label_n': label_n,
                       'label_t': label_t
                   }
                   ,
                   2ä¸ªçº§è”
                   ...
        ]
        è¾“å…¥ï¼š
    '''
    seqs = [t['sequence'] for t in tuples]
    times = [t['time'] for t in tuples]
    lengths = list(map(len, seqs)) #æ¯ä¸ªçº§è”çš„é•¿åº¦
    n_timesteps = max(lengths) #æœ€å¤§é•¿åº¦
    n_samples = len(tuples) #æ ·æœ¬æ•°é‡
    '''try:
        assert n_timesteps == options['seq_len']
    except AssertionError:
        print(n_timesteps, options['seq_len'])'''

    # prepare sequences data
    # ç”Ÿæˆä¸€ä¸ªçº§è”çŸ©é˜µæ•°ç»„ï¼ˆéƒ½æ˜¯0ï¼‰ï¼Œè¡Œæ•°ä¸ºæœ€å¤§é•¿åº¦ï¼Œåˆ—æ•°ä¸ºæ ·æœ¬æ•°é‡ï¼›mï¼ˆçº§è”é•¿åº¦ï¼‰ * nï¼ˆæ¯ä¸€çº§è”ï¼‰
    seqs_matrix = np.zeros((options['seq_len'], n_samples)).astype('int32')
    for i, seq in enumerate(seqs):
        # å°†æ¯ä¸ªçº§è”çš„èŠ‚ç‚¹ç´¢å¼•å¡«å……åˆ°çŸ©é˜µä¸­ï¼Œä»å‰å¾€åå¡«å……æ¯ä¸€åˆ—ï¼Œå°±æ˜¯è¯´ç»™æ¯ä¸€åˆ—å¡«å……ä¸€ä¸ªçº§è”çš„èŠ‚ç‚¹ç´¢å¼•
        seqs_matrix[: lengths[i], i] = seq
    # è½¬ç½®çŸ©é˜µï¼Œå˜æˆm * nçš„çŸ©é˜µ
    seqs_matrix = np.transpose(seqs_matrix)

    # ç”Ÿæˆä¸€ä¸ªæ—¶é—´çŸ©é˜µæ•°ç»„ï¼ˆéƒ½æ˜¯0ï¼‰ï¼ŒåŒç†
    times_matrix = np.zeros((options['seq_len'], n_samples)).astype('float32')
    for i, time in enumerate(times):
        times_matrix[: lengths[i], i] = time
    times_matrix = np.transpose(times_matrix) 

    # prepare topo-masks data
    '''topo_masks = [t['topo_mask'] for t in tuples]
    topo_masks_tensor = np.zeros(
        (n_timesteps, n_samples, n_timesteps)).astype(np.float)
    for i, topo_mask in enumerate(topo_masks):
        topo_masks_tensor[: lengths[i], i, : lengths[i]] = topo_mask'''

    # prepare sequence masksï¼Œæ©ç çŸ©é˜µï¼Œç”¨äºæ ‡è¯†æ¯ä¸ªçº§è”ä¸­æœ‰æ•ˆçš„æ—¶é—´æ­¥ã€‚æ©ç çŸ©é˜µä¸­çš„å…ƒç´ ä¸º 1.0 è¡¨ç¤ºè¯¥æ—¶é—´æ­¥æœ‰æ•ˆï¼Œä¸º 0.0 è¡¨ç¤ºè¯¥æ—¶é—´æ­¥æ— æ•ˆ
    # æ©ç çŸ©é˜µå¯ä»¥å¾ˆå¥½çš„å¤„ç†å˜é•¿åºåˆ—
    len_masks_matrix = np.zeros((n_timesteps, n_samples)).astype(np.float)
    for i, length in enumerate(lengths):
        len_masks_matrix[: length, i] = 1.
    len_masks_matrix = np.transpose(len_masks_matrix)

    # prepare labels data
    '''
        labelæ ‡ç­¾æ˜¯å¹²å˜›çš„ï¼Ÿ

    '''
    if not inference:
        labels_n = [t['label_n'] for t in tuples]
        labels_t = [t['label_t'] for t in tuples]
        labels_vector_n = np.array(labels_n).astype('int32')
        labels_vector_t = np.array(labels_t).astype('int32')
    else:
        labels_vector_t = None
        labels_vector_n = None

    return (seqs_matrix,
            times_matrix,
            len_masks_matrix,
            labels_vector_n, labels_vector_t)


#åŠ è½½å™¨
class Loader:
    def __init__(self, data, options=None):
        self.batch_size = options['batch_size']
        self.idx = 0
        self.data = data
        self.shuffle = True
        self.n = len(data)
        self.n_words = options['node_size'] #æ€»ç»“ç‚¹æ•°
        # ç”Ÿæˆä¸€ä¸ª0åˆ°n-1çš„æ•°ç»„
        self.indices = np.arange(self.n, dtype="int32")
        self.options = options

    def __len__(self):
        return len(self.data) // self.batch_size + 1

    def __call__(self):
        # å¦‚æœshuffleä¸ºTrueå¹¶ä¸”idxä¸º0
        if self.shuffle and self.idx == 0:
            # æ‰“ä¹±æ•°æ®
            np.random.shuffle(self.indices)

        # ç”Ÿæˆä¸€ä¸ªbatchçš„ç´¢å¼•
        batch_indices = self.indices[self.idx: self.idx + self.batch_size]
        # ç”Ÿæˆä¸€ä¸ªbatchçš„æ ·æœ¬
        batch_examples = [self.data[i] for i in batch_indices]

        self.idx += self.batch_size
        if self.idx >= self.n:
            self.idx = 0

        return prepare_minibatch(batch_examples,
                                 inference=False,
                                 options=self.options)


if __name__ == '__main__':
    # print("utils.py ----------")
    load_graph('data//twitter')